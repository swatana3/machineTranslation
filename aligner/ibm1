#!/usr/bin/env python
from __future__ import division
import optparse
import sys
from collections import defaultdict


optparser = optparse.OptionParser()
optparser.add_option("-d", "--data", dest="train", default="data/hansards", help="Data filename prefix (default=data)")
optparser.add_option("-e", "--english", dest="english", default="e", help="Suffix of English filename (default=e)")
optparser.add_option("-f", "--french", dest="french", default="f", help="Suffix of French filename (default=f)")
optparser.add_option("-t", "--threshold", dest="threshold", default=0.8, type="float", help="Threshold for aligning with Dice's coefficient (default=0.5)")
optparser.add_option("-n", "--num_sentences", dest="num_sents", default=sys.maxint, type="int", help="Number of sentences to use for training and alignment")
(opts, _) = optparser.parse_args()
f_data = "%s.%s" % (opts.train, opts.french)
e_data = "%s.%s" % (opts.train, opts.english)

sys.stderr.write("Training with IBM Model1...")

bitext = [[sentence.strip().split() for sentence in pair] for pair in zip(open(f_data), open(e_data))[:opts.num_sents]]
#parameter we are trying to find T-ef
t_ef = defaultdict(int)
#estimating the parameter through count and normalizing by total_f
total_f = defaultdict(int) 
count_ef = defaultdict(int)

#corpus of all f and e 
corpus_e = set()
corpus_f = set()

#initalizing e dict we are going to use for computation with sentences
s_total = defaultdict(int)

#max t_ef, max_j to assign alignment
max_t_ef =0
max_j = 0

#find all the words of f and e
for (n, (f, e)) in enumerate(bitext):
  for f_i in set(f):
    for e_i in set(e):
       corpus_e.add(e_i)
       corpus_f.add(f_i)

#intializing t_ef to the whole corpus uniformly
for f_i in corpus_f:
  for e_i in corpus_e:
    t_ef[(e_i, f_i)] = (1.0/len(corpus_e))

#for loop for while not converged, didn't use threshold just a certain number of for loops
#for this purpose
for f_i in corpus_f:
    for e_i in corpus_e:
      count_ef[(e_i, f_i)] = 0.0
    total_f[f_i] =0.0 
    sys.stderr.write(".")

for x in (0..5): #figure out for loops
  #for all sentence pairs (e,f)
  for (n, (f, e)) in enumerate(bitext): 
    #compute normalization
    for e_i in set(e):
      s_total[e_i] = 0.0
      for f_i in set(f):
        s_total[e_i] += t_ef[(e_i, f_i)]
    #collect count
    for e_i in set(e): #need to know if this e is a string or a set you wnat duplicates
      for f_i in set(f):
        count_ef[(e_i, f_i)] += t_ef[(e_i,f_i)]/s_total[e_i]
        total_f[f_i] += t_ef[(e_i,f_i)] /s_total[e_i]
  
  #
  for f_i in corpus_f:
    for e_i in corpus_e:
      sys.stderr.write(".")
      t_ef[(e_i, f_i)] = count_ef[(e_i, f_i)]/total_f[f_i]

for (f, e) in bitext:

  for (i, f_i) in enumerate(f): 
    max_t_ef = 0
    max_j = 0
    # print "i is: ", i
    # print "f_i is: ", f_i


    # print "max_t_ef before is: ", max_t_ef

    for (j, e_j) in enumerate(e):

      # print "t_ef is: ", t_ef[(e_j,f_i)]

      if (t_ef[(e_j,f_i)] > max_t_ef):
        # print "j_used is: ", j_used
        max_t_ef = t_ef[(e_j,f_i)]
        # print "max_t_ef after is: ", max_t_ef
        max_j = j
        # print "max_j is: ", max_j

    sys.stdout.write("%i-%i " % (i,max_j))
    # j_used.add(max_j)
  sys.stdout.write("\n")